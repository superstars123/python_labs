import re  # –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º–∏


def tokenize(text: str) -> list[str]:
    return re.findall(r"\w+(?:-\w+)*", text)


print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
print(tokenize("2025 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
